Thanks! I’ll look into AI-based methods for parametrizing muon flux from cosmic ray primaries, specifically for IceCube, with a focus on supporting muon bundles and reducing computational cost. I’ll prioritize methods that can be trained on existing simulation data, implemented in PyTorch with Lightning, and are flexible enough to allow for easy model comparison and experimentation.

I’ll get back to you shortly with a detailed recommendation and implementation strategy.


**Short Answer:** For parametrizing atmospheric muon flux at IceCube while supporting muon bundles and reducing computational cost, **Conditional Normalizing Flows (CNFs)**—specifically Neural Spline Flows or Masked Autoregressive Flows—are the optimal choice. Unlike WGAN-GP, they provide exact probability density evaluation essential for likelihood-based reweighting. Unlike diffusion models, they offer efficient single-pass sampling (10⁴× speedup potential) without iterative denoising. Their invertible architecture supports conditional generation based on primary cosmic ray parameters (energy, mass, zenith angle) and integrates natively with PyTorch Lightning for flexible model experimentation at scale.

---

# AI-Driven Parametrization of Atmospheric Muon Flux for IceCube: Methodology Selection and Implementation Framework

## 1. Problem Definition and Current Limitations

### 1.1 Existing Parametrization Methodology

#### 1.1.1 B-Spline Interpolation of CORSIKA Histograms

The current state-of-the-art methodology for atmospheric muon flux parametrization at the IceCube Neutrino Observatory relies on **tensor-product B-spline interpolation** implemented through the Photospline package, which smooths between histogram bins generated by the CORSIKA (COsmic Ray SImulations for KAscade) Monte Carlo framework . This approach processes approximately **25 billion air showers** simulated across primary energies from 600 GeV/nucleon to 100 PeV, utilizing a mixed composition ratio of 10:5:3:2:1 for proton, helium, nitrogen, aluminum, and iron primaries . The dimensional hierarchy of the parametrization spans three dimensions for multiplicity distributions (zenith angle, depth, multiplicity), four dimensions for radial distributions (adding radial distance from shower axis), and five dimensions for energy distributions (incorporating muon energy) .

To stabilize the interpolation, the framework applies specific logarithmic transformations: the flux uses **ln(Φ)** versus **cos(θ)**, depth, and multiplicity; radial distributions employ **ln(dP/dr²)** versus the same variables plus radius; and energy distributions utilize **ln(dP/dE)** versus the full parameter set including **ln(E)** . These transformations convert inherently exponential and power-law physical behaviors into approximately polynomial forms, enabling order-2 regularization that penalizes curvature and prefers straight-line extrapolations in regions of sparse statistics. However, this histogram-based approach suffers from the **curse of dimensionality**, requiring exponential increases in simulation statistics to populate high-dimensional bins adequately, and introduces discretization artifacts at bin boundaries that can systematically bias rare event generation .

#### 1.1.2 Photospline Framework and Manual Fitting Requirements

While Photospline provides superior flexibility compared to rigid analytical forms, it imposes substantial **manual overhead** in the fitting process. Achieving satisfactory fits across the critical depth range of 1.5–2.5 km beneath the Antarctic ice surface demands extensive "manual fiddling" to address mismatches that appear in localized regions of parameter space, particularly at high zenith angles where atmospheric overburden varies dramatically . The regularization parameters must be carefully tuned to balance fidelity against training data with physical plausibility in extrapolation regions, requiring domain expertise and iterative refinement that consumes significant human resources.

Furthermore, the framework creates rigid dependencies on specific CORSIKA simulation configurations. When the collaboration updates hadronic interaction models—transitioning between SIBYLL 2.3c, QGSJET-II, or EPOS-LHC—or explores different atmospheric density profiles, the entire pipeline of histogram generation, spline fitting, and validation must be repeated . The resulting spline tables, while compact compared to raw histograms, still require substantial memory footprints (up to **30 TB for training samples**) and involve recursive basis function evaluations that present computational overhead when sampling billions of events .

#### 1.1.3 Parametric Equations from Becherini et al. and Limitations

Prior to the B-spline era, the collaboration utilized parametric equations based on Becherini et al. (arXiv:0907.5563 and arXiv:0802.0562), which attempted to fit single-muon flux, bundle multiplicity ratios, and normalized radial and energy distributions to relatively simple functional forms comprising polynomials and power laws in depth and zenith angle . While these parametrizations offered compact representations suitable for publication, attempts to refit them to IceCube-specific simulation data revealed **significant limitations** in capturing the complex dependencies at detector depths, yielding unsatisfactory fits that required extensive manual intervention .

These analytical approaches also rely on several **simplifying approximations** that limit their applicability for modern high-precision analyses: azimuthal symmetry around the shower axis (neglecting geomagnetic effects), perfect parallelism of all muons in a bundle, and planar shower fronts without curvature . While reasonable for large-scale simulations, these approximations break down for detailed bundle structure analysis over kilometer-scale baselines, particularly when studying the lateral offsets and early light signatures that distinguish muon bundles from neutrino-induced single muons .

### 1.2 Computational Bottlenecks in Simulation Chains

#### 1.2.1 CORSIKA Air Shower Generation Costs at High Energies

The computational cost of generating atmospheric muons using the full CORSIKA simulation chain presents a **severe bottleneck** for high-energy analyses. For proton-induced showers at 10¹⁵ eV, simulation times range from **7.3 seconds per shower** on modern hardware, extending to hours or days for EeV-range primaries due to the exponential growth in secondary particle multiplicity . The GAIAS2 project quantified this bottleneck, noting that traditional CPU-based simulation of high-energy air showers requires enormous computational resources, with individual events consuming days of processing time .

This inefficiency is compounded by **geometric acceptance**: CORSIKA must track particles from the top of the atmosphere (30–40 km altitude) to ground level, yet only a tiny fraction of simulated showers produce muons reaching the IceCube detector volume. Acceptance fractions often drop below **0.1%** for high-energy, large-zenith-angle events, meaning the simulation wastes 99.9% of computational effort on particles that never contribute to the detectable signal . Current campaigns utilize power-law spectral weighting (E⁻² or E⁻²·⁶) to optimize statistics, but generating sufficient samples for rare event searches (such as prompt atmospheric neutrinos or ultra-high-energy astrophysical neutrinos) remains prohibitively expensive .

#### 1.2.2 PROPOSAL Propagation Overhead

Following CORSIKA generation, the PROPOSAL (Propagator with Optimal Precision and Speed for All Leptons) code handles muon propagation through Antarctic ice, modeling continuous energy losses (ionization, bremsstrahlung, photonuclear interactions, pair production) and stochastic interactions . For high-energy muons traversing kilometers of ice with step sizes constrained by stochastic loss fluctuations, this propagation adds **significant computational overhead**, particularly for high-multiplicity bundles where each muon track must be followed independently through depth-dependent density profiles .

The MuonGun framework attempts to mitigate this by decoupling air shower simulation from ice propagation, pre-computing muon fluxes at cylindrical sampling surfaces surrounding the detector . However, this decoupling creates a rigid boundary: variations in ice models (scattering coefficients, absorption lengths) or atmospheric profiles require **complete regeneration** of the parametrization tables, necessitating reprocessing of billions of CORSIKA showers .

#### 1.2.3 Inefficiency in Targeting Specific Muon Configurations

A fundamental limitation of the current approach is the **lack of biasing mechanisms** to target specific muon configurations. Because simulations start with primary cosmic rays rather than in-ice muons, the characteristics of bundles reaching the detector are emergent properties of stochastic shower development, making it impossible to efficiently generate specific topologies (e.g., single TeV-scale muons from near-horizontal directions or high-multiplicity iron-induced bundles) without simulating enormous numbers of irrelevant primaries . The "forward" simulation approach results in extremely low acceptance rates for rare configurations, forcing analysts to apply complex weighting schemes to sparse simulation samples.

### 1.3 Muon Bundle Modeling Challenges

#### 1.3.1 Multiplicity and Radial Distribution Correlations

Muon bundles—collections of muons from single air showers—exhibit **complex multi-dimensional correlations** between multiplicity (number of muons), radial distribution (distance from shower axis), and energy spectra that challenge simple parametrizations. The multiplicity distribution depends non-linearly on primary energy and mass, with iron nuclei producing significantly higher multiplicities than protons due to larger interaction cross-sections . However, the relationship is stochastic; fluctuations in early shower development cause large event-to-event variance even for fixed primary parameters.

The radial distribution shows **strong correlations with multiplicity and primary mass**: high-multiplicity bundles exhibit broader radial profiles due to increased transverse momentum from multiple hadronic interactions, while energy spectra vary with radial distance (core muons are higher energy, peripheral muons more numerous but softer) . Current B-spline approaches tabulate these as separate histograms, potentially losing critical correlations between energy, radius, and multiplicity that affect Cherenkov light yield superposition in the detector .

#### 1.3.2 Decoupling of Production and Propagation in MuonGun

The MuonGun framework decouples production and propagation by parametrizing muon flux at the detector boundary, enabling **two orders of magnitude speedup** for targeted analyses . However, this decoupling prevents the modeling of correlations between atmospheric production and in-ice propagation. For example, muons produced higher in the atmosphere have different survival probabilities and energy spectra than those produced lower, but the decoupled approach treats these as independent stages .

Furthermore, MuonGun inherits the **limitations of B-spline interpolation**: it cannot easily generate configurations poorly represented in original CORSIKA statistics (rare high-energy bundles or unusual primary compositions), and it lacks differentiability, preventing gradient-based optimization in modern analysis pipelines .

#### 1.3.3 High-Dimensional Output Spaces (Energy, Zenith, Depth, Radius)

The complete characterization requires modeling a **5-dimensional output space**: muon energy (GeV to PeV), zenith angle (0–90°), depth (1–2.8 km), radial distance from shower axis (0 to hundreds of meters), and bundle multiplicity (1 to hundreds) . The curse of dimensionality makes histogram-based approaches exponentially expensive, forcing aggressive simplifications like azimuthal symmetry and factorized distributions that lose correlation information critical for background rejection in neutrino analyses .

### 1.4 Requirements for Next-Generation Parametrization

#### 1.4.1 Density Estimation for Likelihood Evaluation

A critical requirement is **exact probability density evaluation**, not merely sample generation. IceCube analyses rely on unbinned maximum likelihood fitting where the probability density of observed muon configurations must be computable for importance sampling and reweighting . This requirement eliminates pure generative adversarial approaches (which lack tractable densities) and favors architectures like normalizing flows that provide exact likelihoods via the change-of-variables formula .

#### 1.4.2 Conditional Generation Based on Primary Cosmic Ray Parameters

The parametrization must support **conditional generation** based on primary energy (600 GeV–100 PeV), mass composition (p, He, N, Al, Fe), and zenith angle, allowing targeted simulation of specific cosmic ray models (H3a, H4a, GST, GSF) without retraining . This requires architectures that can modulate the output distribution based on continuous and categorical conditioning variables.

#### 1.4.3 Support for Reweighting and Systematic Variations

The model must enable **efficient reweighting** between different hadronic interaction models (SIBYLL, QGSJET, EPOS) and atmospheric profiles without regenerating simulation campaigns. This requires either explicit density ratios or conditional inputs that encode systematic variations, allowing propagation of uncertainties through the simulation chain .

## 2. Comparative Analysis of Generative AI Architectures

| Architecture | Exact Likelihood | Sampling Speed | Training Stability | Conditional Generation | IceCube Precedent | Primary Limitation |
|-------------|------------------|----------------|-------------------|----------------------|-------------------|-------------------|
| **Normalizing Flows** | **Yes** (tractable Jacobian) | **Fast** (single pass) | **High** | **Yes** (CNFs) | **Yes** (Event reconstruction ) | Architecture design complexity |
| **WGAN-GP** | No (requires approximation) | Fast (single pass) | Medium (gradient penalty tuning) | Yes (CWGAN-GP) | Yes (GAIAS2 ) | No native density estimation; mode collapse risk |
| **Diffusion Models** | Indirect (expensive) | **Slow** (100–1000 steps) | **High** | Yes | Emerging  | Prohibitive inference cost for large-scale MC |
| **Discriminative NNs** | No (point estimates) | **Very Fast** | **High** | Limited | Yes (DeepMuon ) | Cannot model stochastic fluctuations |

*Table 1: Comparative analysis of AI architectures for muon flux parametrization. Normalizing flows offer the unique combination of exact likelihood computation, fast sampling, and established IceCube integration.*

### 2.1 Normalizing Flows (NFs)

#### 2.1.1 Exact Likelihood Computation and Density Estimation

Normalizing flows learn an **invertible mapping** $f: \mathcal{Z} \rightarrow \mathcal{X}$ between a simple base distribution $p_Z(\mathbf{z})$ (typically Gaussian) and the complex target distribution $p_X(\mathbf{x})$ through differentiable, bijective transformations. The key advantage is the **change-of-variables formula**, which enables exact computation of the target density:

$$p_X(\mathbf{x}) = p_Z(f^{-1}(\mathbf{x})) \left| \det \frac{\partial f^{-1}}{\partial \mathbf{x}} \right| = p_Z(\mathbf{z}) \left| \det \frac{\partial f}{\partial \mathbf{z}} \right|^{-1}$$

This exact likelihood capability is essential for IceCube's likelihood-based analyses, enabling importance sampling where the flow serves as a proposal distribution with analytically computable weights . Unlike implicit generative models, flows provide **tractable Jacobians** through architectural constraints (triangular or coupling layer structures), allowing both density evaluation and generation within a single framework .

#### 2.1.2 Conditional Variants for Primary Energy and Mass Composition

**Conditional Normalizing Flows (CNFs)** extend the architecture to model $p(\mathbf{x}|\mathbf{c})$ by making transformation parameters depend on conditioning variables $\mathbf{c}$ through neural network layers. For IceCube, $\mathbf{c}$ includes primary energy (log-scaled), mass number (embedded categorical), zenith angle, hadronic model identifier, and atmospheric profile . This enables **amortized inference** across the full cosmic ray parameter space, allowing a single model to generate muon bundles for arbitrary primary spectra without retraining.

The conditioning mechanism supports **continuous interpolation** between discrete categories (e.g., between proton and iron primaries) and facilitates systematic studies by varying conditional inputs. Classifier-free guidance techniques allow trading off between sample diversity and adherence to conditioning, providing physicists fine-grained control over generation fidelity versus physical variance .

#### 2.1.3 IceCube Precedent in Event Reconstruction

The IceCube collaboration has **successfully deployed CNFs** for neutrino event reconstruction, learning per-event posterior distributions over energy and direction from DOM hit patterns . These applications demonstrate the framework's ability to handle high-dimensional, sparse data and model complex, non-Gaussian uncertainties while maintaining coverage guarantees . This precedent establishes infrastructure compatibility (PyTorch, GPU acceleration, IceTray integration) and validates the scalability of flows to IceCube's data volumes and dimensionality .

#### 2.1.4 Invertible Transformations and Differentiable Programming

The **bidirectional invertibility** of flows enables gradients to propagate through the sampling process, supporting differentiable programming for end-to-end optimization of analysis cuts or detector designs . This is impossible with traditional Monte Carlo or B-spline tables. The architecture allows "smart" Monte Carlo where gradients of the likelihood with respect to systematic parameters (cosmic ray spectral index, composition ratios) can be computed efficiently for gradient-based fitting .

#### 2.1.5 Architectural Variants (RealNVP, MAF, Neural Spline Flows)

**RealNVP** (Real-valued Non-Volume Preserving) uses affine coupling layers that split inputs into two halves, applying scale/translation transformations conditioned on one half to the other. While efficient and parallelizable, affine couplings may struggle with multi-modal distributions without deep stacks .

**Masked Autoregressive Flows (MAF)** model each dimension conditioned on all previous dimensions through autoregressive masking, capturing complex dependencies at the cost of slower sequential sampling . For muon bundles where correlations between energy, radius, and multiplicity are crucial, MAF provides superior expressiveness.

**Neural Spline Flows (NSF)** replace affine transformations with **monotonic rational-quadratic splines**, offering enhanced flexibility for modeling sharp peaks and heavy tails (characteristic of power-law energy spectra) while maintaining analytical invertibility . Given the current B-spline methodology, NSF represents a natural neural generalization that preserves the spline inductive bias with learned flexibility.

### 2.2 Wasserstein GAN with Gradient Penalty (WGAN-GP)

#### 2.2.1 GAIAS2 Benchmarks for CORSIKA Acceleration

The **GAIAS2** (Generative AI for Air Shower Simulation) project demonstrated that WGAN-GP with self-attention mechanisms can achieve **10⁴× speedup** over CORSIKA, generating 3×10⁴ showers in under one minute versus weeks of CPU time . Training required approximately 74 hours on an NVIDIA A40 GPU, with an ensemble of 57 networks achieving a Wasserstein distance of 0.04 to training data .

#### 2.2.2 Self-Attention Mechanisms for Shower Physics

Self-attention layers capture **long-range dependencies** in air shower development, relating particles across large spatial separations and energy hierarchies . This is crucial for modeling correlations between high-energy core muons and peripheral shower components.

#### 2.2.3 Generation Quality vs. Density Estimation Trade-offs

Despite impressive speedup, **WGAN-GP lacks tractable density estimation**. The critic network estimates Wasserstein distance but cannot evaluate $p(\mathbf{x})$ for generated samples . This prevents likelihood-based reweighting and importance sampling, requiring kernel density approximations that reintroduce systematic errors. For IceCube's requirement of exact likelihood evaluation for background subtraction, this limitation is prohibitive.

#### 2.2.4 Mode Collapse Risks in High-Dimensional Spaces

GANs are susceptible to **mode collapse**, where the generator ignores rare but physically important configurations (e.g., high-multiplicity iron bundles or prompt muons from charmed decays) . GAIAS2 mitigated this through ensemble methods (57 networks), but this multiplies memory requirements and complexity . In high-dimensional spaces with steeply falling spectra, mode collapse could systematically bias sensitivity calculations for rare astrophysical signals.

### 2.3 Diffusion Models

#### 2.3.1 State-of-the-Art Generation Quality

Diffusion models achieve **state-of-the-art generation quality** by learning to reverse a gradual noising process, capturing fine details of shower development through iterative refinement . Recent "flow matching" variants offer simulation-free training objectives that may improve efficiency .

#### 2.3.2 Density Estimation via Reverse Process Methods

Unlike GANs, diffusion models provide pathways to density estimation through reverse process evaluation or probability flow ODEs. However, computing exact likelihoods requires **integrating over the denoising trajectory**, involving hundreds to thousands of neural network evaluations per sample . This computational overhead is prohibitive for IceCube's requirement of billions of likelihood evaluations in unbinned fits.

#### 2.3.3 Sampling Speed Limitations and Computational Cost

The primary disadvantage is **iterative sampling**: generating samples requires 100–1000 denoising steps, making diffusion models orders of magnitude slower than single-pass generators (flows or GANs) . Even with accelerated sampling (DDIM, DPM-Solver), the per-sample latency makes large-scale Monte Carlo production (billions of events) computationally expensive compared to normalizing flows.

#### 2.3.4 Training Stability Advantages Over GANs

Diffusion models offer **superior training stability**, avoiding the adversarial dynamics and mode collapse of GANs . The denoising objective is well-behaved and provides useful gradients throughout training. However, for the specific IceCube use case where inference speed is as critical as generation quality, this advantage does not offset the sampling bottleneck.

### 2.4 Discriminative Surrogate Approaches

#### 2.4.1 DeepMuon Architecture for Cosmic Muon Generation

The **DeepMuon** project employs deep neural networks with specific preprocessing (Box-Cox transformation with $\lambda=9$, tanh normalization) to generate cosmic muon distributions at depth . This approach transforms the simulation problem into faster shallow-water approximations, achieving significant speedup over GEANT4/CRY propagation.

#### 2.4.2 Direct Regression vs. Probabilistic Modeling

DeepMuon uses **deterministic regression** or implicit generative modeling rather than explicit probabilistic distributions. While fast, this approach **cannot capture stochastic fluctuations** in air shower development—a given primary can produce vastly different muon bundles due to hadronic interaction fluctuations. Without modeling the full conditional distribution $P(\text{bundle}|\text{primary})$, discriminative approaches underestimate variance and fail to reproduce distribution tails critical for background estimation.

#### 2.4.3 Speedup Characteristics and Limitations

While DeepMuon achieves impressive inference speed (comparable to neural generative models), the **lack of density estimation** and inability to model event-by-event stochasticity make it unsuitable as a full replacement for Monte Carlo in likelihood-based analyses. It serves better as a fast filter or pre-selection tool rather than a complete parametrization framework.

## 3. Recommended Architecture: Conditional Normalizing Flows

### 3.1 Theoretical Advantages for Parametrization Tasks

#### 3.1.1 Native Support for Probability Density Evaluation

**Conditional Normalizing Flows (CNFs)** provide the unique combination of **exact density estimation** and **efficient sampling** required to replace B-spline parametrizations. The tractable Jacobian enables direct computation of $P(\text{muon properties} | \text{primary parameters})$, supporting importance sampling where events generated under one hypothesis can be reweighted to another via density ratios . This eliminates the need for multiple simulation campaigns when exploring systematic variations in cosmic ray composition or hadronic models.

The density evaluation also facilitates **gradient-based optimization** of systematic parameters. When fitting the cosmic ray composition model to observed data, gradients of the total likelihood with respect to spectral indices or mass ratios can be backpropagated through the flow, accelerating convergence compared to grid-scan methods .

#### 3.1.2 Efficient Sampling for Monte Carlo Integration

CNFs generate samples through a **single forward pass** through the invertible network, transforming base distribution samples (Gaussian noise) into physical muon configurations. This provides **throughput comparable to B-spline evaluation** (millions of events per minute on modern GPUs) while maintaining the flexibility of neural networks . Unlike diffusion models, there is no iterative denoising overhead; unlike GANs, there is no mode collapse risk requiring ensemble methods.

For large-scale production requiring billions of events, the **embarrassingly parallel** nature of flow sampling (batch processing on GPUs) aligns with IceCube's computational infrastructure. The PyTorch Lightning framework supports distributed data parallel training and inference, allowing the model to scale across multiple nodes for massive event generation campaigns .

#### 3.1.3 Handling of Multi-Modal Distributions in Muon Bundles

Muon bundle distributions are **inherently multi-modal** due to discrete primary mass composition: proton primaries produce distinct multiplicity/radius distributions compared to iron primaries . Normalizing flows, particularly those using **Neural Spline Flows** or deep **Masked Autoregressive Flows**, can model these multi-modal distributions by warping unimodal base distributions into complex, multi-peaked target distributions without assuming factorization between variables.

The conditional architecture $p(\mathbf{x}|\mathbf{c})$ with primary mass $A$ as a conditioning variable allows the model to learn **distinct transformations for each primary type** while sharing parameters across similar masses, enabling interpolation to untrained compositions and smooth transitions between systematic variations .

### 3.2 Input-Output Structure Design

#### 3.2.1 Conditioning Variables (Primary Energy, Zenith Angle, Mass Composition)

The conditional input vector $\mathbf{c}$ should include:

1. **Primary Energy ($E_{\text{primary}}$)**: Logarithmically scaled ($\ln E$) to span 600 GeV–100 PeV uniformly, following power-law weighting ($E^{-2}$ or $E^{-2.6}$) during training to ensure adequate coverage of the dynamic range .

2. **Primary Mass ($A$)**: Categorical variable (proton=1, helium=4, nitrogen=14, aluminum=27, iron=56) encoded as learned embeddings or one-hot vectors, with standard simulation ratios (10:5:3:2:1) .

3. **Zenith Angle ($\theta$)**: Transformed to $\cos\theta$ (0 to 1) to linearize the atmospheric column depth dependence .

4. **Atmospheric Model**: Categorical identifier for seasonal variations (MSIS-90-E based profiles) .

5. **Hadronic Interaction Model**: Categorical variable (SIBYLL 2.3c/d, QGSJET-II, EPOS-LHC) enabling systematic studies by conditioning .

#### 3.2.2 Target Variables (Muon Energy, Direction, Multiplicity, Radial Distance)

The output space $\mathbf{x}$ should generate complete muon bundle descriptions:

1. **Multiplicity ($N_\mu$)**: Discrete number of muons, modeled as categorical or discretized continuous with appropriate rounding during sampling. Distributions follow approximately power-law behavior with cutoffs dependent on primary energy and mass .

2. **Radial Distance ($r$)**: Distance from shower axis, generated as $r^2$ or $\ln r$ to match current B-spline transformations .

3. **Muon Energy ($E_\mu$)**: Individual muon energies spanning GeV to PeV, transformed using $\ln E_\mu$ or Box-Cox ($\lambda=9$) followed by tanh normalization to handle steep power-law spectra .

4. **Direction**: Zenith and azimuth relative to shower axis. While current parametrizations assume perfect parallelism, the flow can model small angular deviations due to multiple scattering.

5. **Depth**: Vertical evaluation depth (1–2.8 km in 100 m steps) .

For variable-length bundles, use **autoregressive generation** (sampling one muon at a time conditioned on previous muons) or **set-based architectures** (DeepSets, PointNet) to handle variable $N_\mu$.

#### 3.2.3 Variable Transformations (Logarithmic Scaling, Angular Periodicity)

**Critical preprocessing** ensures stable training:

- **Energy variables**: Apply $\ln E$ to compress dynamic range and render power-law distributions approximately Gaussian.
- **Radial distance**: Use $\ln(dP/dr^2)$ transformation to linearize the exponential falloff of muon density with radius .
- **Angular variables**: Use $\cos\theta$ for zenith to handle periodicity; represent azimuth as $(\sin\phi, \cos\phi)$ pairs to avoid discontinuities at $2\pi$ boundaries.
- **Flux values**: Transform to $\ln\Phi$ to handle exponential variations with depth and angle .

These transformations mirror the current B-spline methodology, ensuring the flow learns distributions with approximately linear trends in transformed coordinates, facilitating stable training with order-2 regularization or spline-based coupling layers.

### 3.3 Architectural Specifics

#### 3.3.1 Masked Autoregressive Flows (MAF) for High-Dimensional Correlations

**MAF** is recommended for capturing **complex intra-bundle correlations**. By modeling $p(x_i | x_{<i})$, MAF ensures that each muon's properties depend on all previously generated muons in the bundle, capturing the physical constraint that energy sharing among muons is conserved and that radial positions correlate with energies .

For a bundle of maximum multiplicity $N_{\max}$, the autoregressive ordering should place **multiplicity first**, followed by energy-radius pairs for each muon (sorted by descending energy to match physical shower development). While sampling is sequential (slower than parallel coupling layers), density evaluation remains fast, and the expressiveness is critical for accurate bundle topology modeling.

Architecture details: Use **20 MAF blocks** (following ATLAS Drell-Yan precedent ), each with 2-layer dense networks of **128–512 hidden units** (scaled by available compute), ReLU activations, and batch normalization. Given the user's substantial compute resources, deeper networks (256–512 units) can capture finer correlations without overfitting on the billions of training events available from CORSIKA.

#### 3.3.2 Neural Spline Flows for Non-Linear Dependencies

**Neural Spline Flows (NSF)** should be used as the coupling layer transformation rather than affine couplings. NSF uses **monotonic rational-quadratic splines** with $K$ bins (typically $K=8$–$16$) to model the transformation function, providing:

- **Multi-modal flexibility**: Can model sharp peaks (high-energy muons) and heavy tails (low-energy population) simultaneously.
- **Analytical invertibility**: Splines are invertible via binary search or analytical root-finding, maintaining exact likelihood computation.
- **Natural extrapolation**: Matches the spline-based inductive bias of current Photospline methods while learning optimal knot placements.

For IceCube, NSF coupling layers with **8–16 spline bins** provide sufficient capacity to model the steeply falling energy spectra ($E^{-3.7}$ at surface) and exponential radial profiles without the rigidity of fixed-knot B-splines.

#### 3.3.3 Graph Neural Network Integration for Bundle Topology

For modeling **variable-size bundles** and explicit spatial correlations, integrate **Graph Neural Networks (GNNs)** as the conditioner in the flow. Represent each muon as a node with features $(E, r, \theta, \phi)$, and use message passing to capture collective effects:

- **Node features**: Initial embedding of muon properties.
- **Edge features**: Spatial distances between muons or angular separations.
- **Global features**: Primary cosmic ray parameters (energy, mass, zenith) conditioning the entire graph.

Use **Graph Attention Networks (GAT)** or **Edge-Conditioned Convolutions** to allow the flow to focus on physically relevant correlations (e.g., high-energy muons in the core vs. low-energy halo muons). The GNN outputs parameters for the spline transformations in the coupling layers, enabling the flow to respect permutation invariance of muons within bundles while maintaining autoregressive ordering for generation.

### 3.4 Integration with PyTorch Lightning

#### 3.4.1 Modular Trainer Configuration for Model Flexibility

**PyTorch Lightning** provides the ideal framework for the user's requirement to test various model types flexibly. Structure the codebase with **modular coupling layers**:

```python
# Conceptual structure
class MuonFlow(pl.LightningModule):
    def __init__(self, coupling_type='NSF', n_layers=20, hidden_dim=256):
        self.transforms = nn.ModuleList([
            NSF_Coupling(dim, hidden_dim) if coupling_type == 'NSF' 
            else MAF_Coupling(dim, hidden_dim)
            for _ in range(n_layers)
        ])
```

This allows **A/B testing** of MAF vs. NSF vs. RealNVP architectures using identical data pipelines and hyperparameter configurations. The `LightningModule` encapsulates forward (density evaluation) and inverse (sampling) passes, with automatic differentiation handling the Jacobian computations.

#### 3.4.2 Distributed Data Parallel for Large-Scale Training

Given the **25 billion shower dataset**  and complex models (20+ layers, 512 hidden units), utilize **Distributed Data Parallel (DDP)** across multiple GPUs/nodes. Lightning's DDP implementation handles gradient synchronization automatically, enabling scaling to hundreds of GPUs without code modifications .

**Batch size scaling**: With large compute resources, use batch sizes of **1024–4096 events** to reduce gradient variance and accelerate convergence. Ensure batches are stratified across primary energy and mass composition to prevent mode collapse in specific regions.

#### 3.4.3 Checkpointing and Hyperparameter Management

Leverage Lightning's **callback system** for:
- **Model checkpointing**: Save top-k models based on validation negative log-likelihood.
- **Hyperparameter logging**: Integration with Weights & Biases or TensorBoard to track experiments comparing coupling types, layer depths, and conditioning mechanisms.
- **Early stopping**: Monitor validation NLL with patience=20 epochs to prevent overfitting.

Given the computational cost of CORSIKA data generation, robust checkpointing ensures training progress is preserved, while automated hyperparameter search (Optuna integration) can optimize the number of spline bins, hidden layer sizes, and learning rates.

## 4. Implementation Framework

### 4.1 Data Pipeline Architecture

#### 4.1.1 CORSIKA Simulation Data Ingestion and Preprocessing

Ingest raw CORSIKA output (particle lists at observation level) and PROPOSAL propagation results. Extract:
- **Primary parameters**: Energy, mass ID, zenith, azimuth, interaction model.
- **Muon bundles**: For each shower, collect all muons reaching the sampling surface (cylinder at 1–2.8 km depth) with properties $(E, \theta, \phi, r, t)$.

Apply **logarithmic transformations** immediately: $\ln E_{\text{primary}}$, $\ln E_\mu$, $\ln r$ (or $r^2$), and $\cos\theta$ conversions. Normalize all variables to zero mean, unit variance based on training set statistics.

#### 4.1.2 Muon Bundle Feature Extraction and Labeling

For bundle-level modeling:
- Compute **multiplicity** $N_\mu$ (number of muons above threshold).
- Sort muons by descending energy to establish autoregressive ordering.
- Compute **shower core position** and radial distances $r_i$ for each muon.
- Label with primary cosmic ray parameters for conditioning.

Handle **variable-length bundles** by padding to $N_{\max}$ (e.g., 100 muons) with masking, or use set-based architectures for variable output.

#### 4.1.3 Training, Validation, and Test Set Stratification

**Stratified sampling** is critical due to the steeply falling spectrum:
- **Energy bins**: Logarithmic bins from 600 GeV to 100 PeV, ensuring equal representation across decades.
- **Mass composition**: Maintain 10:5:3:2:1 ratio in all splits .
- **Zenith angles**: Ensure coverage from vertical (0°) to horizontal (90°), with oversampling of high-zenith regions where statistics are naturally sparse.

Reserve **10% for validation, 10% for testing**, ensuring no overlap in primary particle IDs to prevent data leakage.

### 4.2 Model Development Strategy

#### 4.2.1 Base Distribution Selection (Gaussian Mixtures vs. Standard Normal)

While standard $\mathcal{N}(0, I)$ is typical, consider **Gaussian mixture models** as the base distribution to capture discrete populations (e.g., conventional vs. prompt muons, or different primary mass signatures). A mixture of 3–5 components can accelerate convergence for multi-modal target distributions.

Alternatively, use **Student-t distributions** with low degrees of freedom (ν=3–5) to handle heavy tails in energy spectra without transformation.

#### 4.2.2 Coupling Layer Design and Neural Network Backbones

For **NSF coupling layers**:
- **Spline bins**: $K=8$–$16$ rational-quadratic bins per dimension.
- **Hidden networks**: 2–3 layer MLPs with 256–512 hidden units (ReLU or Swish activation) predicting spline parameters (widths, heights, derivatives).
- **Conditioning**: Concatenate primary parameters $\mathbf{c}$ to the hidden network input.

For **MAF**:
- **Autoregressive masks**: Ensure strict ordering (multiplicity → muon 1 energy → muon 1 radius → ...).
- **Hidden units**: 512–1024 units per layer given compute availability, with residual connections to prevent vanishing gradients in deep stacks (20+ layers).

#### 4.2.3 Conditioning Mechanisms (Concatenation vs. Feature-wise Transformations)

Implement **Feature-wise Linear Modulation (FiLM)** for robust conditioning:
- Generate scale $\gamma$ and shift $\beta$ parameters from primary features $\mathbf{c}$ via a small MLP.
- Apply $\gamma \odot h + \beta$ to intermediate activations $h$ in the coupling networks.

This provides more expressive conditioning than simple concatenation, allowing the model to modulate the entire distribution shape based on primary mass and energy.

### 4.3 Training Infrastructure

#### 4.3.1 Loss Function Design (Negative Log-Likelihood Optimization)

Optimize the **negative log-likelihood (NLL)**:
$$\mathcal{L} = -\mathbb{E}_{\mathbf{x}, \mathbf{c}} \left[ \ln p_Z(f^{-1}(\mathbf{x}; \mathbf{c})) + \sum_{l=1}^{L} \ln \left| \det \frac{\partial f_l}{\partial \mathbf{z}_l} \right| \right]$$

For numerical stability:
- Compute log-determinants directly (sum of log-diagonal elements for triangular Jacobians).
- Use **mixed precision** (FP16 forward, FP32 backward) with gradient scaling to prevent underflow.

#### 4.3.2 Learning Rate Scheduling and Gradient Clipping

- **Learning rate**: Start at $10^{-3}$ with **cosine annealing** to $10^{-5}$ over 200–300 epochs.
- **Warmup**: 5-epoch linear warmup to prevent early instability.
- **Gradient clipping**: Clip norm to 1.0–5.0 to handle exploding Jacobians in deep flows.

#### 4.3.3 Early Stopping Criteria and Model Selection

Monitor:
- **Validation NLL**: Primary metric for model selection.
- **Wasserstein distance**: Between generated and true distributions for key variables (energy, radius).
- **Coverage**: Fraction of validation events falling within 90% credible intervals of the flow (should be ~90%).

Stop when validation NLL plateaus for 20 consecutive epochs.

### 4.4 Computational Optimization

#### 4.4.1 GPU Utilization for Batch Processing

- **Batch size**: 2048–4096 events per GPU (scaled by memory).
- **Data loading**: Use PyTorch DataLoader with `num_workers=8` and `pin_memory=True` to saturate GPU utilization.
- **Vectorization**: Implement coupling layers to process entire batches simultaneously, utilizing GPU tensor cores for matrix operations.

#### 4.4.2 Mixed Precision Training for Memory Efficiency

Use **Automatic Mixed Precision (AMP)** with `torch.cuda.amp`:
- Forward passes in FP16 for speed.
- Critical operations (Jacobian determinants, log-probabilities) in FP32 for stability.
- This reduces memory footprint by ~50%, enabling larger models or batch sizes on A100/H100 GPUs.

#### 4.4.3 Distributed Training Across Multiple Nodes

Deploy on **4–8 nodes** with 8×A100 GPUs each (32–64 GPUs total):
- **DDP**: Lightning handles gradient synchronization.
- **Global batch size**: 65k–262k events (32 GPUs × 2k–8k per GPU).
- **Checkpointing**: Save every 10 epochs to distributed storage (Lustre/WEKA).

## 5. Validation and Performance Benchmarking

### 5.1 Physics Fidelity Metrics

#### 5.1.1 One-Dimensional Distribution Comparisons (Kolmogorov-Smirnov Tests)

Compare generated vs. CORSIKA distributions using:
- **KS statistics**: For $\ln E_\mu$, $\cos\theta$, $\ln r$, and multiplicity $N_\mu$.
- **Target**: KS statistic $<0.05$ (5% significance) for all 1D marginals.
- **Power-law indices**: Verify spectral indices of energy distributions match CORSIKA within 1%.

#### 5.1.2 Multi-Dimensional Correlation Preservation (Energy-Angle Relationships)

- **Scatter plots**: $E_\mu$ vs. $\cos\theta$ for different primary energies.
- **Correlation coefficients**: Pearson and Spearman correlations between energy and radius should match within 5%.
- **Conditional distributions**: $P(E_\mu | N_\mu=1)$ vs. $P(E_\mu | N_\mu>10)$ should show correct hardening/softening trends.

#### 5.1.3 Muon Bundle Multiplicity and Spatial Distribution Accuracy

- **Multiplicity spectrum**: Verify power-law behavior $dN/dN_\mu \propto N_\mu^{-\alpha}$ with $\alpha \approx 2.0$–$2.5$ depending on primary .
- **Lateral distribution function**: $\rho(r) \propto r^{-\beta}$ with $\beta \approx 0.5$–$1.0$ at large radii.
- **Bundle diameter**: Median diameter ~15 m with exponential tail .

### 5.2 Computational Performance

#### 5.2.1 Inference Speedup Relative to CORSIKA Simulation

**Target metrics**:
- **Generation**: $10^4$× speedup (match GAIAS2 benchmark ): generate $3 \times 10^4$ showers in $<1$ minute on single A100.
- **Density evaluation**: $<1$ ms per event (faster than B-spline table lookup).

#### 5.2.2 Sampling Throughput for Large-Scale Monte Carlo Production

- **Throughput**: $10^6$–$10^7$ events/second on 8×A100 node (batch processing).
- **Scaling**: Linear scaling with GPU count up to 64 GPUs (tested via DDP).

#### 5.2.3 Memory Footprint and Scalability Analysis

- **Model size**: 50–200 MB (compressed) for 20-layer NSF/MAF with 512 hidden units.
- **Inference memory**: 2–4 GB GPU memory for batch size 4096.
- **Training memory**: 40–80 GB per GPU (A100 80GB recommended for largest models).

### 5.3 Systematic Uncertainty Quantification

#### 5.3.1 Calibration of Density Estimates on Holdout Data

- **Probability integral transform**: Histogram of $CDF(x_i)$ for holdout data should be uniform $U(0,1)$.
- **Calibration plots**: Binned accuracy of density estimates vs. true histogram densities.

#### 5.3.2 Propagation of Primary Cosmic Ray Model Uncertainties

Test reweighting by conditioning on different primary models:
- Generate samples with H3a composition, reweight to H4a using density ratios.
- Verify reweighted distributions match direct H4a generation within statistical errors.

#### 5.3.3 Comparison Against B-Spline Baseline Parametrizations

- **Likelihood comparison**: Flow should achieve higher log-likelihood on test data than B-spline interpolation (better fit).
- **Tail behavior**: Flow should capture high-multiplicity tails ($N_\mu > 50$) more accurately than B-splines (which smooth due to regularization).

## 6. Extensions and Future Directions

### 6.1 Hybrid Architectures

#### 6.1.1 Flow Matching for Continuous Normalizing Flows

Explore **flow matching** or **continuous normalizing flows (CNFs)** using neural ODEs. These learn the vector field transporting base to target distributions without specifying discrete coupling layers, potentially offering:
- **Greater expressiveness** for complex shower physics.
- **Adaptive computation**: Trade accuracy for speed via ODE solver tolerance.

#### 6.1.2 Diffusion-Normalizing Flow Hybrids for Enhanced Sampling

Investigate **distillation** approaches where a diffusion model trains a normalizing flow student, combining the training stability of diffusion with the sampling speed of flows. This could capture rare high-energy tails more accurately while maintaining $10^4$× speedup.

### 6.2 Full Simulation Replacement

#### 6.2.1 End-to-End Air Shower Surrogate Modeling

Extend the flow to generate **full air shower profiles** (electromagnetic, hadronic, and muonic components) at ground level, not just muons. This would replace CORSIKA entirely for detector simulation, with the flow conditioned on primary parameters and generating complete particle lists.

#### 6.2.2 Integration with Detector Response Simulation (CLSim)

Couple the muon flow directly to **CLSim** (Cherenkov light simulation) in a differentiable pipeline:
- Flow generates muon tracks → CLSim propagates photons → Loss computed on DOM hits.
- Enable **end-to-end optimization** of analysis cuts by backpropagating from detector response to primary parameters.

### 6.3 Alternative Applications

#### 6.3.1 Neutrino Event Background Estimation

Use the flow to generate **atmospheric neutrino backgrounds** by modeling the hadronic cascade and neutrino production, enabling joint modeling of muon bundles and neutrino fluxes for unified background estimation in point-source searches.

#### 6.3.2 Cosmic Ray Composition Analysis via Muon Bundle Signatures

Invert the flow for **simulation-based inference**: given observed muon bundle properties $(E, r, N_\mu)$, use the inverse mapping to infer posterior distributions over primary mass $A$ and energy $E_{\text{primary}}$, enabling high-statistics cosmic ray composition analysis without expensive unfolding procedures.